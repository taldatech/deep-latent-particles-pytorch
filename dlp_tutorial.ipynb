{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369899b7",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/fluency/96/000000/particles.png\" style=\"height:50px;display:inline\"> Deep Latent Particles - Tutorial\n",
    "---\n",
    "\n",
    "Tal Daniel\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://colab.research.google.com/github/taldatech/deep-latent-particles-pytorch/blob/main/dlp_tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</center>\n",
    "\n",
    "* Paper: [**Unsupervised Image Representation Learning with Deep Latent Particles**, Tal Daniel and Aviv Tamar, ICML 2022](https://arxiv.org/)\n",
    "* GitHub: <a href=\"https://github.com/taldatech/deep-latent-particles-pytorch\">deep-latent-particles-pytorch</a>\n",
    "* Wepage: <a href=\"https://taldatech.github.io/deep-latent-particles-web/\">ICML 2022 - Deep Latent Particles</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65753934",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/taldatech/deep-latent-particles-web/main/assets/clevrer_manip_1.gif\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bfcc4",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/96/000000/loading.png\" style=\"height:50px;display:inline\"> Running Instructions\n",
    "---\n",
    "* This Jupyter Notebook can be opened locally with Anaconda, or online via Google Colab.\n",
    "* To run online, go to https://colab.research.google.com/ and drag-and-drop the `dlp_tutorial.ipynb` file.\n",
    "    * On Colab, note the \"directory\" icon on the left, logs, figures and checkpoints are saved in this directory.\n",
    "* To run the training on the image dataset, it is better to have a GPU. In Google Cola select `Runtime->Change runtime type->GPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558396b",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/96/000000/software-installer.png\" style=\"height:50px;display:inline\"> Requirements\n",
    "---\n",
    "* If running locally, make sure to set up the environment, preferably with <a href=\"https://anaconda.org/\">Anaconda</a> by running `conda env create -f environment17/19.yml` or with `pip` by running `pip install - r requirements17/19.txt`.\n",
    "* If running online on <a href=\"https://colab.research.google.com\">Google Colab</a>, run the following cell to clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20fd2fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'deep-latent-particles-pytorch'...\n",
      "remote: Enumerating objects: 155, done.\u001b[K\n",
      "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
      "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
      "remote: Total 155 (delta 67), reused 127 (delta 43), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (155/155), 18.21 MiB | 9.09 MiB/s, done.\n",
      "Resolving deltas: 100% (67/67), done.\n"
     ]
    }
   ],
   "source": [
    "# only run this cell if running on Google Colab\n",
    "!git clone https://github.com/taldatech/deep-latent-particles-pytorch.git\n",
    "!cd deep-latent-particles-pytorch\n",
    "!pip install -r requirements19.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef333e",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* [Variational Autoencoders (VAEs)](#-Variational-Autoencoders-(VAEs))\n",
    "* [Soft-IntroVAE Objectives](#-Soft-IntroVAE-Objectives)\n",
    "* [Image Generation Experiments](#-Image-Generation-Experiments)\n",
    "    * [Image Generation Experiments - Architectures](#Image-Generation-Experiments---Architectures)\n",
    "    * [Image Generation Experiments - Algorithm and Train Function](#Image-Generation-Experiments---Algorithm-and-Train-Function)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import torch.optim as optim\n",
    "# model\n",
    "from models import KeyPointVAE\n",
    "# util functions\n",
    "# from utils.util_func import reparameterize, get_kp_mask_from_gmap, create_masks_fast,\n",
    "#plot_keypoints_on_image_batch, create_masks_fast, prepare_logdir, save_config, log_line,\\\n",
    "#     plot_bb_on_image_batch_from_masks_nms\n",
    "from utils.util_func import *\n",
    "from utils.loss_functions import ChamferLossKL, calc_kl, calc_reconstruction_loss, VGGDistance\n",
    "# datasets\n",
    "from datasets.shapes_ds import generate_shape_dataset_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30b711",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/mention.png\" style=\"height:50px;display:inline\"> Introduction\n",
    "---\n",
    "* In this tutorial, we will give a brief overview of the deep latent particles (DLP) framework. \n",
    "* DLP is a new representation of visual data that disentangles object position from appearance. \n",
    "* DLP decomposes the visual input into low-dimensional latent ``particles'', where each particle is described by its spatial location ($z_p$) and features ($z_{\\alpha}$) of its surrounding region. $$ \\{(z_p^i, z_{\\alpha}^i) \\}_{i=1}^K $$\n",
    "* DLP is trained with a variational autoencoder (VAE) objective with a special prior for particle positions based on a spatial-softmax architecture, and a modification of the evidence lower bound (ELBO) loss inspired by the Chamfer distance between particles.\n",
    "* DLP comes in two flavors depending on the scene type: **(1) Masked model** and **(2) Object-based model**.\n",
    "* **Masked model**: designed for non-object scenes (e.g., faces from CelebA), PointNet++ and Gaussian maps model the local regions around the particles, and the rest (e.g., the background) is propagated from the encdoer ($\\Phi_{bypass}$). \n",
    "* **Object-based model**: designed for object-based scenes (e.g., CLEVRER), PointNet++ models the global regions (e.g., the background) and Gaussian maps (optionally) and a separate Glimpse decoder model the objects and their masks.\n",
    "* In this tutorial, *we will focus on the object-based model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/external-icongeek26-linear-colour-icongeek26/64/000000/external-shapes-graphic-design-icongeek26-linear-colour-icongeek26.png\" style=\"height:50px;display:inline\"> Random Shapes Dataset\n",
    "---\n",
    "* First, we will take a look at data for the tutorial: images of random shapes of different sizes and colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d7734568ac4865914d344fa5109269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_images: <torch.utils.data.dataset.TensorDataset object at 0x00000190288A3F88>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_images = generate_shape_dataset_torch(img_size=64, num_images=5)  # [5, 3, 64, 64]\n",
    "print(f'sample_images: {sample_images}')\n",
    "# plot\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(len(sample_images)):\n",
    "    ax = fig.add_subplot(1, len(sample_images), i + 1)\n",
    "    ax.imshow(sample_images[i][0].permute(1, 2, 0).data.cpu())\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/external-flaticons-flat-flat-icons/64/000000/external-neural-network-the-future-flaticons-flat-flat-icons.png\" style=\"height:50px;display:inline\"> The Model\n",
    "---\n",
    "* The model has 3 main components: encoder network, prior network, and decoder network(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "---\n",
    "<img src=\"https://raw.githubusercontent.com/taldatech/deep-latent-particles-web/main/assets/dlp_encoder.gif\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is composed of two components:\n",
    "1. **Position encoder**: outputs keypoints -- the spatial location $z_p = (x,y)$ of interesting areas, where $(x,y)$ are the coordinates of pixels.\n",
    "2. **Appearance encoder**: extracts patches (or glimpses) of pre-determined size centered around $z_p$ and encodes them to latent variables $z_{\\alpha}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE - DO NOT RUN\n",
    "# posterior\n",
    "enc_out = self.encode(x, return_heatmap=True, mask=mask)  # position encoder\n",
    "mu, logvar, kp_heatmap, mu_features, logvar_features, obj_on, _ = enc_out\n",
    "# obj_on is a transperency/presence variable, valeus between [0, 1]\n",
    "# kp_heatmap is a low dimension featuer map, output of the cnn - used to extract visual features later\n",
    "# mu/logvar_features = None [placeholders]\n",
    "# sample\n",
    "z = reparameterize(mu, logvar)\n",
    "# convert [x, y] to gaussian maps (gmaps) -- will be used in the decoder\n",
    "gmap_1_fg = self.to_gauss_map(z[:, :-1], self.features_dim, self.features_dim)\n",
    "# convert gmaps to binary mask -- to cover up parts of the visual feature maps\n",
    "fg_masks_sep = get_kp_mask_from_gmap(gmap_1_fg, threshold=self.mask_threshold, binary=True, elementwise=True).detach()\n",
    "fg_masks = fg_masks_sep.sum(1, keepdim=True).clamp(0, 1)\n",
    "bg_masks = 1 - fg_masks\n",
    "masks_sep = torch.cat([fg_masks_sep, bg_masks], dim=1)\n",
    "# extract visual features around the particles\n",
    "feat_source = x if self.use_object_dec else kp_heatmap.detach()\n",
    "# feat_source is the source from which we extract regions (x is the original image)\n",
    "obj_enc_out = self.encode_object_features_sep(feat_source, z[:, :-1], kp_heatmap.detach(), masks_sep.detach())\n",
    "mu_features, logvar_features, cropped_objects = obj_enc_out[0], obj_enc_out[1], obj_enc_out[2]\n",
    "# note that the last particle is reserved for the bg and is located at (0, 0) [the center of the image]\n",
    "# this is why we use z[:, :-1] when we extract features for the particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior\n",
    "---\n",
    "* The prior addresses the question: what are the interesting areas in the image?\n",
    "* It constrains the model such that the output of the position encoder is actually spatial positions and not just random values between $[-1, 1]$.\n",
    "* We extract points-of-intereset in the image by applying spatial-Softmax (SSM) over feature maps extracted from **patches** in the image. \n",
    "* We term the set of extracted prior keypoints as keypoint proposals.\n",
    "* Since we output one keypoint per patch, we may end up with a large set of keypoints, not all are informative.\n",
    "* To filter out some proposal keypoints, we either randomly sample from this set or use a heuristic where we take the top-K most distant keypoints from the center of the patch.\n",
    "* The reasoning behind the heuristic is that SSM is based on averaging, and contant areas in the image (e.g., a solid color patch) are not informative and SSM will result in a keypoint in the center of the patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE - DO NOT RUN\n",
    "# prior\n",
    "kp_p = self.prior(x_prior, global_kp=True)  \n",
    "# global_kp = True returns the coordinates in the whole image. Otherwise, returns the local coordinates inside the patch.\n",
    "kp_p = kp_p.view(x_prior.shape[0], -1, 2)  # [batch_size, n_kp_total, 2]\n",
    "# filter proposals by distance to the patches' center\n",
    "dist_from_center = self.prior.get_distance_from_patch_centers(kp_p, global_kp=True)\n",
    "_, indices = torch.topk(dist_from_center, k=self.n_kp_prior, dim=-1, largest=True)\n",
    "batch_indices = torch.arange(kp_p.shape[0]).view(-1, 1).to(kp_p.device)\n",
    "kp_p = kp_p[batch_indices, indices]\n",
    "# random cnn features for prior also work\n",
    "kp_p = kp_p.detach()\n",
    "# alternatively, just sample random kp\n",
    "# kp_p = kp_p[:, torch.randperm(kp_p.shape[1])[:self.n_kp_prior]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "---\n",
    "<img src=\"https://raw.githubusercontent.com/taldatech/deep-latent-particles-web/main/assets/dlp_decoder.gif\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the decoder for the **object-based** model PointNet++ models the global regions (e.g., the background) and Gaussian maps (optionally) and a separate Glimpse decoder model the objects and their masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE - DO NOT RUN\n",
    "# decoder\n",
    "gmap_1_fg = self.to_gauss_map(z[:, :-1], self.features_dim, self.features_dim)\n",
    "gmap_1_bg = 1 - gmap_1_fg.sum(1, keepdim=True).clamp(0, 1).detach()\n",
    "gmap_1 = torch.cat([gmap_1_fg, gmap_1_bg], dim=1)\n",
    "fg_masks_sep = get_kp_mask_from_gmap(gmap_1_fg, threshold=self.mask_threshold, binary=True, elementwise=True).detach()\n",
    "fg_masks = fg_masks_sep.sum(1, keepdim=True).clamp(0, 1)\n",
    "bg_masks = 1 - fg_masks\n",
    "masks = torch.cat([fg_masks.expand_as(gmap_1_fg), bg_masks], dim=1)\n",
    "gmap_2 = self.pointnet(position=z.detach(), features=torch.cat([z.detach(), z_features], dim=-1))\n",
    "gmap = torch.cat([gmap_1[:, :-1], gmap_2], dim=1)\n",
    "rec = self.dec(gmap)  # bg elements\n",
    "# decode object and translate them to the positions of the keypoints\n",
    "object_dec_out = self.decode_objects(z, z_features, obj_on, bg=rec)\n",
    "dec_objects, dec_objects_trans, rec = object_dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it All Together\n",
    "---\n",
    "<img src=\"https://raw.githubusercontent.com/taldatech/deep-latent-particles-web/main/assets/dlp_arch_all.PNG\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <img src=\"https://img.icons8.com/external-icongeek26-linear-colour-icongeek26/64/000000/external-shapes-graphic-design-icongeek26-linear-colour-icongeek26.png\" style=\"height:50px;display:inline\"> Training and Optimization\n",
    "---\n",
    "The model is optimized as a variational autoencoder (VAE) with objective of maximizing the evidence lower bound (ELBO): $$ \\log p_\\theta(x) \\geq \\mathbb{E}_{q(z|x)} \\left[\\log p_\\theta(x|z)\\right] - KL(q(z|x) \\Vert p(z)) \\doteq ELBO(x) $$\n",
    "The ELBO is decomposed to the <i>reconstruction error</i> and a <i>KL-divergence</i> regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, here we have two **unordered sets** (or point clouds) of position latent variables: the posterior keypoints and the keypoint proposals from the prior. Note that *the number of points in each set may also differ*.\n",
    "\n",
    "\n",
    "The Chamfer distance between two sets $S_1$ and $S_2$: \n",
    "$$ d_{CH}(S_1, S_2) = \\sum_{x \\in S_1}\\min_{y \\in S_2}||x-y||_2^2 + \\sum_{y \\in S_2}\\min_{x \\in S_1}||x-y||_2^2. $$\n",
    "<img src=\"https://raw.githubusercontent.com/taldatech/deep-latent-particles-web/main/assets/chamfer_distance.gif\" style=\"height:300px\">\n",
    "\n",
    "Animation by <a href=\"https://www.youtube.com/watch?v=P4IyrsWicfs\">Luke Hawkes</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose the <b>Chamfer-KL</b>, a <i>novel modification</i> for the KL term:\n",
    "$$ d_{CH-KL}(S_1, S_2) = \\sum_{x \\in S_1}\\min_{y \\in S_2}KL(x \\Vert y) + \\sum_{y \\in S_2}\\min_{x \\in S_1}KL(x \\Vert y). $$\n",
    "Note that the Chamfer-KL is not a metric and maintains the properties of the standard KL term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dlp(ds=\"shapes\", batch_size=16, lr=5e-4, device=torch.device(\"cpu\"), kp_activation=\"none\",\n",
    "              pad_mode='replicate', num_epochs=250, load_model=False, n_kp=8, recon_loss_type=\"mse\",\n",
    "              use_logsoftmax=False, sigma=0.1, beta_kl=1.0, beta_rec=1.0, dropout=0.0,\n",
    "              dec_bone=\"gauss_pointnetpp\", patch_size=16, topk=15, n_kp_enc=20, eval_epoch_freq=5,\n",
    "              learned_feature_dim=0, n_kp_prior=100, weight_decay=0.0, kp_range=(0, 1),\n",
    "              run_prefix=\"\", mask_threshold=0.2, use_tps=False, use_pairs=False, use_object_enc=True,\n",
    "              use_object_dec=False, warmup_epoch=5, iou_thresh=0.2, anchor_s=0.25, learn_order=False,\n",
    "              kl_balance=0.1, exclusive_patches=False):\n",
    "    \"\"\"\n",
    "    ds: dataset name (str)\n",
    "    enc_channels: channels for the posterior CNN (takes in the whole image)\n",
    "    prior_channels: channels for prior CNN (takes in patches)\n",
    "    n_kp: number of kp to extract from each (!) patch\n",
    "    n_kp_prior: number of kp to filter from the set of prior kp (of size n_kp x num_patches)\n",
    "    n_kp_enc: number of posterior kp to be learned (this is the actual number of kp that will be learnt)\n",
    "    use_logsoftmax: for spatial-softmax, set True to use log-softmax for numerical stability\n",
    "    pad_mode: padding for the CNNs, 'zeros' or  'replicate' (default)\n",
    "    sigma: the prior std of the KP\n",
    "    dropout: dropout for the CNNs. We don't use it though...\n",
    "    dec_bone: decoder backbone -- \"gauss_pointnetpp_feat\": Masked Model, \"gauss_pointnetpp\": Object Model\n",
    "    patch_size: patch size for the prior KP proposals network (not to be confused with the glimpse size)\n",
    "    kp_range: the range of keypoints, can be [-1, 1] (default) or [0,1]\n",
    "    learned_feature_dim: the latent visual features dimensions extracted from glimpses.\n",
    "    kp_activation: the type of activation to apply on the keypoints: \"tanh\" for kp_range [-1, 1], \"sigmoid\" for [0, 1]\n",
    "    mask_threshold: activation threshold (>thresh -> 1, else 0) for the binary mask created from the Gaussian-maps.\n",
    "    anchor_s: defines the glimpse size as a ratio of image_size (e.g., 0.25 for image_size=128 -> glimpse_size=32)\n",
    "    learn_order: experimental feature to learn the order of keypoints - but it doesn't work yet.\n",
    "    use_object_enc: set True to use a separate encoder to encode visual features of glimpses.\n",
    "    use_object_dec: set True to use a separate decoder to decode glimpses (Object Model).\n",
    "    iou_thresh: intersection-over-union threshold for non-maximal suppression (nms) to filter bounding boxes\n",
    "    use_tps: set True to use a tps augmentation on the input image for datasets that support this option\n",
    "    use_pairs: for CelebA dataset, set True to use a tps-augmented image for the prior.\n",
    "    topk: the number top-k particles with the lowest variance (highest confidence) to filter for the plots.\n",
    "    warmup_epoch: (used for the Object Model) number of epochs where only the object decoder is trained.\n",
    "    recon_loss_type: tpe of pixel reconstruction loss (\"mse\", \"vgg\").\n",
    "    beta_rec: coefficient for the reconstruction loss (we use 1.0).\n",
    "    beta_kl: coefficient for the KL divergence term in the loss.\n",
    "    kl_balance: coefficient for the balance between the ChamferKL (for the KP)\n",
    "                and the standard KL (for the visual features),\n",
    "                kl_loss = beta_kl * (chamfer_kl + kl_balance * kl_features)\n",
    "    exclusive_patches: (mostly) enforce one particle pre object by masking up regions that were already encoded.\n",
    "    \"\"\"\n",
    "\n",
    "    # load data\n",
    "    if ds == \"shapes\":\n",
    "        image_size = 64\n",
    "        ch = 3\n",
    "        enc_channels = [32, 64, 128]\n",
    "        prior_channels = (16, 32, 64)\n",
    "        print('generating random shapes dataset')\n",
    "        dataset = generate_shape_dataset_torch(num_images=40_000)\n",
    "        milestones = (20, 40, 80)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # save hyper-parameters\n",
    "    hparams = {'ds': ds, 'batch_size': batch_size, 'lr': lr, 'kp_activation': kp_activation, 'pad_mode': pad_mode,\n",
    "               'num_epochs': num_epochs, 'n_kp': n_kp, 'recon_loss_type': recon_loss_type,\n",
    "               'use_logsoftmax': use_logsoftmax, 'sigma': sigma, 'beta_kl': beta_kl, 'beta_rec': beta_rec,\n",
    "               'dec_bone': dec_bone, 'patch_size': patch_size, 'topk': topk, 'n_kp_enc': n_kp_enc,\n",
    "               'eval_epoch_freq': eval_epoch_freq, 'learned_feature_dim': learned_feature_dim,\n",
    "               'n_kp_prior': n_kp_prior, 'weight_decay': weight_decay, 'kp_range': kp_range,\n",
    "               'run_prefix': run_prefix, 'mask_threshold': mask_threshold, 'use_tps': use_tps, 'use_pairs': use_pairs,\n",
    "               'use_object_enc': use_object_enc, 'use_object_dec': use_object_dec, 'warmup_epoch': warmup_epoch,\n",
    "               'iou_thresh': iou_thresh, 'anchor_s': anchor_s, 'learn_order': learn_order, 'kl_balance': kl_balance,\n",
    "               'milestones': milestones, 'image_size': image_size, 'enc_channels': enc_channels,\n",
    "               'prior_channels': prior_channels, 'exclusive_patches': exclusive_patches}\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0, pin_memory=True,\n",
    "                            drop_last=True)\n",
    "    # model\n",
    "    model = KeyPointVAE(cdim=ch, enc_channels=enc_channels, prior_channels=prior_channels,\n",
    "                        image_size=image_size, n_kp=n_kp, learned_feature_dim=learned_feature_dim,\n",
    "                        use_logsoftmax=use_logsoftmax, pad_mode=pad_mode, sigma=sigma,\n",
    "                        dropout=dropout, dec_bone=dec_bone, patch_size=patch_size, n_kp_enc=n_kp_enc,\n",
    "                        n_kp_prior=n_kp_prior, kp_range=kp_range, kp_activation=kp_activation,\n",
    "                        mask_threshold=mask_threshold, use_object_enc=use_object_enc,\n",
    "                        exclusive_patches=exclusive_patches, use_object_dec=use_object_dec, anchor_s=anchor_s,\n",
    "                        learn_order=learn_order).to(device)\n",
    "\n",
    "    logvar_p = torch.log(torch.tensor(sigma ** 2)).to(device)  # logvar of the constant std -> for the kl\n",
    "    # prepare saving location\n",
    "    run_name = f'{ds}_dlp_{dec_bone}' + run_prefix\n",
    "    log_dir = prepare_logdir(runname=run_name, src_dir='./')\n",
    "    fig_dir = os.path.join(log_dir, 'figures')\n",
    "    save_dir = os.path.join(log_dir, 'saves')\n",
    "    save_config(log_dir, hparams)\n",
    "\n",
    "    kl_loss_func = ChamferLossKL(use_reverse_kl=False)\n",
    "    if recon_loss_type == \"vgg\":\n",
    "        recon_loss_func = VGGDistance(device=device)\n",
    "    else:\n",
    "    recon_loss_func = calc_reconstruction_loss\n",
    "    betas = (0.9, 0.999)\n",
    "    eps = 1e-4\n",
    "    # we use separate optimizers for the encoder and decoder, but it is not really necessary...\n",
    "    optimizer_e = optim.Adam(model.get_parameters(encoder=True, prior=True, decoder=False), lr=lr, betas=betas, eps=eps,\n",
    "                             weight_decay=weight_decay)\n",
    "    optimizer_d = optim.Adam(model.get_parameters(encoder=False, prior=False, decoder=True), lr=lr, betas=betas,\n",
    "                             eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler_e = optim.lr_scheduler.MultiStepLR(optimizer_e, milestones=milestones, gamma=0.5)\n",
    "    scheduler_d = optim.lr_scheduler.MultiStepLR(optimizer_d, milestones=milestones, gamma=0.5)\n",
    "\n",
    "    if load_model:\n",
    "        try:\n",
    "            model.load_state_dict(\n",
    "                torch.load(os.path.join(save_dir, f'{ds}_dlp_{dec_bone}.pth'), map_location=device))\n",
    "            print(\"loaded model from checkpoint\")\n",
    "        except:\n",
    "            print(\"model checkpoint not found\")\n",
    "\n",
    "    # statistics\n",
    "    losses = []\n",
    "    losses_rec = []\n",
    "    losses_kl = []\n",
    "    losses_kl_kp = []\n",
    "    losses_kl_feat = []\n",
    "\n",
    "    # save PSNR values of the reconstruction\n",
    "    psnrs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        batch_losses_rec = []\n",
    "        batch_losses_kl = []\n",
    "        batch_losses_kl_kp = []\n",
    "        batch_losses_kl_feat = []\n",
    "        batch_psnrs = []\n",
    "        pbar = tqdm(iterable=dataloader)\n",
    "        for batch in pbar:\n",
    "            if ds == 'shapes':\n",
    "                x = batch[0].to(device)\n",
    "                x_prior = x\n",
    "            else:\n",
    "                x = batch.to(device)\n",
    "                x_prior = x\n",
    "            batch_size = x.shape[0]\n",
    "            # forward pass\n",
    "            noisy_masks = (epoch < 5 * warmup_epoch)  # add small noise to the alpha masks\n",
    "            model_output = model(x, x_prior=x_prior, warmup=(epoch < warmup_epoch), noisy_masks=noisy_masks)\n",
    "            mu_p = model_output['kp_p']\n",
    "            gmap = model_output['gmap']\n",
    "            mu = model_output['mu']\n",
    "            logvar = model_output['logvar']\n",
    "            rec_x = model_output['rec']\n",
    "            mu_features = model_output['mu_features']\n",
    "            logvar_features = model_output['logvar_features']\n",
    "            # object stuff\n",
    "            dec_objects_original = model_output['dec_objects_original']\n",
    "            cropped_objects_original = model_output['cropped_objects_original']\n",
    "            obj_on = model_output['obj_on']  # [batch_size, n_kp]\n",
    "\n",
    "            # reconstruction error\n",
    "            if use_object_dec and dec_objects_original is not None and epoch < warmup_epoch:\n",
    "                # reconstruct patches in the warmup stage\n",
    "                if recon_loss_type == \"vgg\":\n",
    "                    _, dec_objects_rgb = torch.split(dec_objects_original, [1, 3], dim=2)\n",
    "                    dec_objects_rgb = dec_objects_rgb.reshape(-1, *dec_objects_rgb.shape[2:])\n",
    "                    cropped_objects_original = cropped_objects_original.reshape(-1,\n",
    "                                                                                *cropped_objects_original.shape[2:])\n",
    "                    # vgg has a minimal input size, so we interpolate if the patch is too small\n",
    "                    if cropped_objects_original.shape[-1] < 32:\n",
    "                        cropped_objects_original = F.interpolate(cropped_objects_original, size=32, mode='bilinear',\n",
    "                                                                 align_corners=False)\n",
    "                        dec_objects_rgb = F.interpolate(dec_objects_rgb, size=32, mode='bilinear',\n",
    "                                                        align_corners=False)\n",
    "                    loss_rec_obj = recon_loss_func(cropped_objects_original, dec_objects_rgb, reduction=\"mean\")\n",
    "\n",
    "                else:\n",
    "                    _, dec_objects_rgb = torch.split(dec_objects_original, [1, 3], dim=2)\n",
    "                    dec_objects_rgb = dec_objects_rgb.reshape(-1, *dec_objects_rgb.shape[2:])\n",
    "                    cropped_objects_original = cropped_objects_original.clone().reshape(-1,\n",
    "                                                                                        *cropped_objects_original.shape[\n",
    "                                                                                         2:])\n",
    "                    loss_rec_obj = calc_reconstruction_loss(cropped_objects_original, dec_objects_rgb,\n",
    "                                                            loss_type='mse', reduction='mean')\n",
    "                loss_rec = loss_rec_obj\n",
    "            else:\n",
    "                # reconstruct full image\n",
    "                if recon_loss_type == \"vgg\":\n",
    "                    loss_rec = recon_loss_func(x, rec_x, reduction=\"mean\")\n",
    "                else:\n",
    "                    loss_rec = calc_reconstruction_loss(x, rec_x, loss_type='mse', reduction='mean')\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    psnr = -10 * torch.log10(F.mse_loss(rec_x, x))\n",
    "                    batch_psnrs.append(psnr.data.cpu().item())\n",
    "\n",
    "            # kl-divergence\n",
    "            logvar_kp = logvar_p.expand_as(mu_p)\n",
    "\n",
    "            # the final kp is the bg kp which is located in the center (so no need for it)\n",
    "            # to reproduce the results on celeba, use `mu_post = mu`, `logvar_post = logvar`\n",
    "            mu_post = mu[:, :-1]\n",
    "            logvar_post = logvar[:, :-1]\n",
    "            # mu_post = mu\n",
    "            # logvar_post = logvar\n",
    "            mu_prior = mu_p\n",
    "            logvar_prior = logvar_kp\n",
    "\n",
    "            loss_kl_kp = kl_loss_func(mu_preds=mu_post, logvar_preds=logvar_post, mu_gts=mu_prior,\n",
    "                                      logvar_gts=logvar_prior).mean()\n",
    "\n",
    "            if learned_feature_dim > 0:\n",
    "                loss_kl_feat = calc_kl(logvar_features.view(-1, logvar_features.shape[-1]),\n",
    "                                       mu_features.view(-1, mu_features.shape[-1]), reduce='none')\n",
    "                loss_kl_feat = loss_kl_feat.view(batch_size, n_kp_enc + 1).sum(1).mean()\n",
    "            else:\n",
    "                loss_kl_feat = torch.tensor(0.0, device=device)\n",
    "            loss_kl = loss_kl_kp + kl_balance * loss_kl_feat\n",
    "\n",
    "            loss = beta_rec * loss_rec + beta_kl * loss_kl\n",
    "            # backprop\n",
    "            optimizer_e.zero_grad()\n",
    "            optimizer_d.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_e.step()\n",
    "            optimizer_d.step()\n",
    "            # log\n",
    "            batch_losses.append(loss.data.cpu().item())\n",
    "            batch_losses_rec.append(loss_rec.data.cpu().item())\n",
    "            batch_losses_kl.append(loss_kl.data.cpu().item())\n",
    "            batch_losses_kl_kp.append(loss_kl_kp.data.cpu().item())\n",
    "            batch_losses_kl_feat.append(loss_kl_feat.data.cpu().item())\n",
    "            # progress bar\n",
    "            if use_object_dec and epoch < warmup_epoch:\n",
    "                pbar.set_description_str(f'epoch #{epoch} (warmup)')\n",
    "            elif use_object_dec and noisy_masks:\n",
    "                pbar.set_description_str(f'epoch #{epoch} (noisy masks)')\n",
    "            else:\n",
    "                pbar.set_description_str(f'epoch #{epoch}')\n",
    "            pbar.set_postfix(loss=loss.data.cpu().item(), rec=loss_rec.data.cpu().item(),\n",
    "                             kl=loss_kl.data.cpu().item())\n",
    "        pbar.close()\n",
    "        losses.append(np.mean(batch_losses))\n",
    "        losses_rec.append(np.mean(batch_losses_rec))\n",
    "        losses_kl.append(np.mean(batch_losses_kl))\n",
    "        losses_kl_kp.append(np.mean(batch_losses_kl_kp))\n",
    "        losses_kl_feat.append(np.mean(batch_losses_kl_feat))\n",
    "        if len(batch_psnrs) > 0:\n",
    "            psnrs.append(np.mean(batch_psnrs))\n",
    "        # keep track of bounding box scores to set a hard threshold (as bb scores are not normalized)\n",
    "        # epoch_bb_scores = torch.cat(batch_bb_scores, dim=0)\n",
    "        # bb_mean_score = epoch_bb_scores.mean().data.cpu().item()\n",
    "        # bb_mean_scores.append(bb_mean_score)\n",
    "        # schedulers\n",
    "        scheduler_e.step()\n",
    "        scheduler_d.step()\n",
    "        # epoch summary\n",
    "        log_str = f'epoch {epoch} summary for dec backbone: {dec_bone}\\n'\n",
    "        log_str += f'loss: {losses[-1]:.3f}, rec: {losses_rec[-1]:.3f}, kl: {losses_kl[-1]:.3f}\\n'\n",
    "        log_str += f'kl_balance: {kl_balance:.4f}, kl_kp: {losses_kl_kp[-1]:.3f}, kl_feat: {losses_kl_feat[-1]:.3f}\\n'\n",
    "        log_str += f'mu max: {mu.max()}, mu min: {mu.min()}\\n'\n",
    "        if ds != 'celeba':\n",
    "            log_str += f'val loss (freq: {eval_epoch_freq}): {valid_loss:.3f},' \\\n",
    "                       f' best: {best_valid_loss:.3f} @ epoch: {best_valid_epoch}\\n'\n",
    "        if obj_on is not None:\n",
    "            log_str += f'obj_on max: {obj_on.max()}, obj_on min: {obj_on.min()}\\n'\n",
    "        if len(psnrs) > 0:\n",
    "            log_str += f'mean psnr: {psnrs[-1]:.3f}\\n'\n",
    "        print(log_str)\n",
    "        log_line(log_dir, log_str)\n",
    "\n",
    "        if epoch % eval_epoch_freq == 0 or epoch == num_epochs - 1:\n",
    "            max_imgs = 8\n",
    "            img_with_kp = plot_keypoints_on_image_batch(mu[:, :-1].clamp(min=kp_range[0], max=kp_range[1]), x, radius=3,\n",
    "                                                        thickness=1, max_imgs=max_imgs, kp_range=kp_range)\n",
    "            img_with_kp_p = plot_keypoints_on_image_batch(mu_p, x_prior, radius=3, thickness=1, max_imgs=max_imgs,\n",
    "                                                          kp_range=kp_range)\n",
    "            # top-k\n",
    "            with torch.no_grad():\n",
    "                logvar_sum = logvar[:, :-1].sum(-1)\n",
    "                logvar_topk = torch.topk(logvar_sum, k=topk, dim=-1, largest=False)\n",
    "                indices = logvar_topk[1]  # [batch_size, topk]\n",
    "                batch_indices = torch.arange(mu.shape[0]).view(-1, 1).to(mu.device)\n",
    "                topk_kp = mu[batch_indices, indices]\n",
    "                # bounding boxes\n",
    "                masks = create_masks_fast(mu[:, :-1].detach(), anchor_s=model.anchor_s, feature_dim=x.shape[-1])\n",
    "                masks = torch.where(masks < mask_threshold, 0.0, 1.0)\n",
    "                bb_scores = -1 * logvar_sum\n",
    "                hard_threshold = bb_scores.mean()\n",
    "            if use_object_dec:\n",
    "                img_with_masks_nms, nms_ind = plot_bb_on_image_batch_from_masks_nms(masks, x, scores=bb_scores,\n",
    "                                                                                    iou_thresh=iou_thresh,\n",
    "                                                                                    thickness=1, max_imgs=max_imgs,\n",
    "                                                                                    hard_thresh=hard_threshold)\n",
    "                # hard_thresh: a general threshold for bb scores (set None to not use it)\n",
    "                bb_str = f'bb scores: max: {bb_scores.max():.2f}, min: {bb_scores.min():.2f},' \\\n",
    "                         f' mean: {bb_scores.mean():.2f}\\n'\n",
    "                print(bb_str)\n",
    "            log_line(log_dir, bb_str)\n",
    "            img_with_kp_topk = plot_keypoints_on_image_batch(topk_kp.clamp(min=kp_range[0], max=kp_range[1]), x,\n",
    "                                                             radius=3, thickness=1, max_imgs=max_imgs,\n",
    "                                                             kp_range=kp_range)\n",
    "            if use_object_dec and dec_objects_original is not None:\n",
    "                dec_objects = model_output['dec_objects']\n",
    "                vutils.save_image(torch.cat([x[:max_imgs, -3:], img_with_kp[:max_imgs, -3:].to(device),\n",
    "                                             rec_x[:max_imgs, -3:], img_with_kp_p[:max_imgs, -3:].to(device),\n",
    "                                             img_with_kp_topk[:max_imgs, -3:].to(device),\n",
    "                                             dec_objects[:max_imgs, -3:],\n",
    "                                             img_with_masks_nms[:max_imgs, -3:].to(device)],\n",
    "                                            dim=0).data.cpu(), '{}/image_{}.jpg'.format(fig_dir, epoch),\n",
    "                                  nrow=8, pad_value=1)\n",
    "                with torch.no_grad():\n",
    "                    _, dec_objects_rgb = torch.split(dec_objects_original, [1, 3], dim=2)\n",
    "                    dec_objects_rgb = dec_objects_rgb.reshape(-1, *dec_objects_rgb.shape[2:])\n",
    "                    cropped_objects_original = cropped_objects_original.clone().reshape(-1, 3,\n",
    "                                                                                        cropped_objects_original.shape[\n",
    "                                                                                            -1],\n",
    "                                                                                        cropped_objects_original.shape[\n",
    "                                                                                            -1])\n",
    "                    if cropped_objects_original.shape[-1] != dec_objects_rgb.shape[-1]:\n",
    "                        cropped_objects_original = F.interpolate(cropped_objects_original,\n",
    "                                                                 size=dec_objects_rgb.shape[-1],\n",
    "                                                                 align_corners=False, mode='bilinear')\n",
    "                vutils.save_image(\n",
    "                    torch.cat([cropped_objects_original[:max_imgs * 2, -3:], dec_objects_rgb[:max_imgs * 2, -3:]],\n",
    "                              dim=0).data.cpu(), '{}/image_obj_{}.jpg'.format(fig_dir, epoch),\n",
    "                    nrow=8, pad_value=1)\n",
    "            else:\n",
    "                vutils.save_image(torch.cat([x[:max_imgs, -3:], img_with_kp[:max_imgs, -3:].to(device),\n",
    "                                             rec_x[:max_imgs, -3:], img_with_kp_p[:max_imgs, -3:].to(device),\n",
    "                                             img_with_kp_topk[:max_imgs, -3:].to(device)],\n",
    "                                            dim=0).data.cpu(), '{}/image_{}.jpg'.format(fig_dir, epoch),\n",
    "                                  nrow=8, pad_value=1)\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(save_dir, f'{ds}_dlp_{dec_bone}{run_prefix}.pth'))\n",
    "        # plot graphs\n",
    "        if epoch > 0:\n",
    "            num_plots = 3\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(num_plots, 1, 1)\n",
    "            ax.plot(np.arange(len(losses[1:])), losses[1:], label=\"loss\")\n",
    "            ax.set_title(run_name)\n",
    "            ax.legend()\n",
    "\n",
    "            ax = fig.add_subplot(num_plots, 1, 2)\n",
    "            ax.plot(np.arange(len(losses_kl[1:])), losses_kl[1:], label=\"kl\", color='red')\n",
    "            if learned_feature_dim > 0:\n",
    "                ax.plot(np.arange(len(losses_kl_kp[1:])), losses_kl_kp[1:], label=\"kl_kp\", color='cyan')\n",
    "                ax.plot(np.arange(len(losses_kl_feat[1:])), losses_kl_feat[1:], label=\"kl_feat\", color='green')\n",
    "            ax.legend()\n",
    "\n",
    "            ax = fig.add_subplot(num_plots, 1, 3)\n",
    "            ax.plot(np.arange(len(losses_rec[1:])), losses_rec[1:], label=\"rec\", color='green')\n",
    "            ax.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{fig_dir}/{run_name}_graph.jpg')\n",
    "            plt.close('all')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "# default hyper-parameters\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 100\n",
    "load_model = False\n",
    "eval_epoch_freq = 2\n",
    "n_kp = 1  # num kp per patch\n",
    "mask_threshold = 0.2  # mask threshold for the features from the encoder\n",
    "kp_range = (-1, 1)\n",
    "weight_decay = 0.0\n",
    "run_prefix = \"\"\n",
    "learn_order = False\n",
    "use_logsoftmax = False\n",
    "pad_mode = 'replicate'\n",
    "sigma = 0.1  # default sigma for the gaussian maps\n",
    "dropout = 0.0\n",
    "kp_activation = \"tanh\"\n",
    "# ds specific hp\n",
    "ds = \"shapes\"\n",
    "beta_kl = 0.05\n",
    "beta_rec = 1.0\n",
    "n_kp_enc = 10  # total kp to output from the encoder / filter from prior\n",
    "n_kp_prior = 15\n",
    "patch_size = 8\n",
    "learned_feature_dim = 6  # additional features than x,y for each kp\n",
    "dec_bone = \"gauss_pointnetpp\"\n",
    "topk = min(10, n_kp_enc)  # display top-10 kp with smallest variance\n",
    "recon_loss_type = \"mse\"\n",
    "use_tps = False\n",
    "use_pairs = False\n",
    "use_object_enc = True  # separate object encoder\n",
    "use_object_dec = True  # separate object decoder\n",
    "warmup_epoch = 1\n",
    "anchor_s = 0.25\n",
    "kl_balance = 0.001\n",
    "exclusive_patches = True\n",
    "\n",
    "model = train_dlp(ds=ds, batch_size=batch_size, lr=lr,\n",
    "                      device=device, num_epochs=num_epochs, kp_activation=kp_activation,\n",
    "                      load_model=load_model, n_kp=n_kp, use_logsoftmax=use_logsoftmax, pad_mode=pad_mode,\n",
    "                      sigma=sigma, beta_kl=beta_kl, beta_rec=beta_rec, dropout=dropout, dec_bone=dec_bone,\n",
    "                      kp_range=kp_range, learned_feature_dim=learned_feature_dim, weight_decay=weight_decay,\n",
    "                      recon_loss_type=recon_loss_type, patch_size=patch_size, topk=topk, n_kp_enc=n_kp_enc,\n",
    "                      eval_epoch_freq=eval_epoch_freq, n_kp_prior=n_kp_prior, run_prefix=run_prefix,\n",
    "                      mask_threshold=mask_threshold, use_tps=use_tps, use_pairs=use_pairs, anchor_s=anchor_s,\n",
    "                      use_object_enc=use_object_enc, use_object_dec=use_object_dec, exclusive_patches=exclusive_patches,\n",
    "                      warmup_epoch=warmup_epoch, learn_order=learn_order, kl_balance=kl_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e3035",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
